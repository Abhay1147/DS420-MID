{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f496b176-8f80-4598-9b84-a680a01dbae3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'sgd_best_model.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Load models\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m sgd_model \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msgd_best_model.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m rf_model \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrf_best_model.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Load and prepare test data\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/DS420/lib/python3.10/site-packages/joblib/numpy_pickle.py:735\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode, ensure_native_byte_order)\u001b[0m\n\u001b[1;32m    733\u001b[0m         obj \u001b[38;5;241m=\u001b[39m _unpickle(fobj, ensure_native_byte_order\u001b[38;5;241m=\u001b[39mensure_native_byte_order)\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 735\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    736\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _validate_fileobject_and_memmap(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m (\n\u001b[1;32m    737\u001b[0m             fobj,\n\u001b[1;32m    738\u001b[0m             validated_mmap_mode,\n\u001b[1;32m    739\u001b[0m         ):\n\u001b[1;32m    740\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    741\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[1;32m    742\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[1;32m    743\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sgd_best_model.pkl'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                           f1_score, roc_auc_score, confusion_matrix, \n",
    "                           classification_report, roc_curve, precision_recall_curve)\n",
    "import joblib\n",
    "\n",
    "# Load models\n",
    "sgd_model = joblib.load('sgd_best_model.pkl')\n",
    "rf_model = joblib.load('rf_best_model.pkl')\n",
    "\n",
    "# Load and prepare test data\n",
    "from hr_analytics_preparation import prepare_data\n",
    "df = pd.read_csv('HR_data.csv')\n",
    "X_train, X_test, y_train, y_test, preprocessor = prepare_data(df)\n",
    "\n",
    "print(\"MODEL COMPARISON ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Evaluate both models\n",
    "models = {\n",
    "    'Mini-batch GD Logistic Regression': sgd_model,\n",
    "    'Random Forest': rf_model\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nEvaluating {name}...\")\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    results[name] = {\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred),\n",
    "        'Recall': recall_score(y_test, y_pred),\n",
    "        'F1-Score': f1_score(y_test, y_pred),\n",
    "        'ROC-AUC': roc_auc_score(y_test, y_pred_proba)\n",
    "    }\n",
    "    \n",
    "    print(f\"Accuracy: {results[name]['Accuracy']:.4f}\")\n",
    "    print(f\"Precision: {results[name]['Precision']:.4f}\")\n",
    "    print(f\"Recall: {results[name]['Recall']:.4f}\")\n",
    "    print(f\"F1-Score: {results[name]['F1-Score']:.4f}\")\n",
    "    print(f\"ROC-AUC: {results[name]['ROC-AUC']:.4f}\")\n",
    "\n",
    "# Create comparison dataframe\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df = results_df.round(4)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPREHENSIVE MODEL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(results_df)\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Model Comparison - HR Analytics Employee Turnover Prediction', fontsize=16)\n",
    "\n",
    "# 1. Metrics comparison\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
    "x_pos = np.arange(len(metrics))\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    row, col = i // 3, i % 3\n",
    "    values = [results[model][metric] for model in models.keys()]\n",
    "    bars = axes[row, col].bar(models.keys(), values, alpha=0.7, color=['skyblue', 'lightcoral'])\n",
    "    axes[row, col].set_title(f'{metric} Comparison')\n",
    "    axes[row, col].set_ylabel(metric)\n",
    "    axes[row, col].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, values):\n",
    "        axes[row, col].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                           f'{value:.4f}', ha='center', va='bottom')\n",
    "\n",
    "# 2. ROC Curve\n",
    "plt.figure(figsize=(10, 8))\n",
    "for name, model in models.items():\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC = {auc_score:.4f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 3. Precision-Recall Curve\n",
    "plt.figure(figsize=(10, 8))\n",
    "for name, model in models.items():\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    plt.plot(recall, precision, label=f'{name}')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 4. Confusion Matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "for idx, (name, model) in enumerate(models.items()):\n",
    "    y_pred = model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx])\n",
    "    axes[idx].set_title(f'Confusion Matrix - {name}')\n",
    "    axes[idx].set_xlabel('Predicted')\n",
    "    axes[idx].set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"METRICS PRIORITIZATION EXPLANATION\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "For employee turnover prediction, we prioritize the following metrics:\n",
    "\n",
    "1. RECALL: Most important metric because we want to identify as many employees \n",
    "   who might leave as possible (minimize false negatives). Missing an employee \n",
    "   who will leave (false negative) is more costly than incorrectly predicting \n",
    "   someone will leave (false positive).\n",
    "\n",
    "2. F1-Score: Balances precision and recall, giving us a single metric to \n",
    "   evaluate model performance considering both false positives and false negatives.\n",
    "\n",
    "3. ROC-AUC: Measures the model's ability to distinguish between classes across \n",
    "   all classification thresholds.\n",
    "\n",
    "4. PRECISION: Important but secondary - we want our predictions of who will \n",
    "   leave to be accurate, but it's better to be cautious and flag potential \n",
    "   leavers even if we're not 100% sure.\n",
    "\n",
    "BUSINESS IMPACT:\n",
    "- High Recall: Prevents surprise resignations, allows proactive retention efforts\n",
    "- Moderate Precision: Avoids wasting too many resources on false alarms\n",
    "- Overall: Better to be safe than sorry in employee retention\n",
    "\"\"\")\n",
    "\n",
    "# Final recommendation\n",
    "best_model = results_df.loc[results_df['Recall'].idxmax()]\n",
    "print(f\"\\nRECOMMENDED MODEL: {results_df['Recall'].idxmax()}\")\n",
    "print(f\"Reason: Highest Recall ({best_model['Recall']}) - most important for employee retention\")\n",
    "print(f\"Additional strength: F1-Score of {best_model['F1-Score']}\")\n",
    "\n",
    "# Save comparison results\n",
    "results_df.to_csv('model_comparison_results.csv')\n",
    "print(\"\\nComparison results saved to 'model_comparison_results.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb39d53b-a315-4436-8a07-be628839f4dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
